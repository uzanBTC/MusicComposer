"""
éŸ³ä¹ç”Ÿæˆæ ¸å¿ƒæ¨¡å—
è´Ÿè´£æ¨¡å‹åŠ è½½å’ŒéŸ³ä¹ç”Ÿæˆ
"""
import torch
import numpy as np
import scipy.io.wavfile as wavfile
from transformers import AutoProcessor, MusicgenForConditionalGeneration
from pathlib import Path
from typing import Optional, Tuple
import time
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config.config import (
    MODEL_NAME, 
    DEFAULT_MAX_TOKENS, 
    DEFAULT_GUIDANCE_SCALE,
    PROMPT_STEP1,
    PROMPT_STEP2
)
from src.utils import get_device


class MusicGenerator:
    """éŸ³ä¹ç”Ÿæˆå™¨"""
    
    def __init__(self, model_name: str = MODEL_NAME, device: Optional[str] = None):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡ï¼ˆNone åˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
        """
        self.model_name = model_name
        self.device = device or get_device()
        self.processor = None
        self.model = None
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if self.model is not None:
            print("æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡")
            return
        
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        start_time = time.time()
        
        self.processor = AutoProcessor.from_pretrained(self.model_name)
        self.model = MusicgenForConditionalGeneration.from_pretrained(self.model_name)
        self.model = self.model.to(self.device)
        
        load_time = time.time() - start_time
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ˆè€—æ—¶: {load_time:.2f}ç§’ï¼‰")
        
    def generate_from_single(
        self,
        audio_data: np.ndarray,
        sample_rate: int,
        prompt: str,
        max_tokens: int = DEFAULT_MAX_TOKENS,
        guidance_scale: float = DEFAULT_GUIDANCE_SCALE
    ) -> np.ndarray:
        """
        åŸºäºå•ä¸ªéŸ³é¢‘ç”Ÿæˆ
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            sample_rate: é‡‡æ ·ç‡
            prompt: æ–‡å­—æç¤º
            max_tokens: ç”Ÿæˆé•¿åº¦
            guidance_scale: å¼•å¯¼ç³»æ•°
            
        Returns:
            ç”Ÿæˆçš„éŸ³é¢‘æ•°æ®
        """
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")
        
        print(f"ğŸµ å¼€å§‹ç”Ÿæˆï¼ˆæç¤º: {prompt}ï¼‰")
        
        # å‡†å¤‡è¾“å…¥
        inputs = self.processor(
            audio=audio_data,
            sampling_rate=sample_rate,
            text=[prompt],
            padding=True,
            return_tensors="pt",
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ç”Ÿæˆ
        start_time = time.time()
        with torch.no_grad():
            audio_values = self.model.generate(
                **inputs,
                do_sample=True,
                guidance_scale=guidance_scale,
                max_new_tokens=max_tokens
            )
        
        gen_time = time.time() - start_time
        print(f"âœ… ç”Ÿæˆå®Œæˆï¼ˆè€—æ—¶: {gen_time:.2f}ç§’ï¼‰")
        
        # è½¬æ¢ä¸º numpy
        audio_values = audio_values.cpu().numpy()
        return audio_values[0, 0]
    
    def generate_from_fusion(
        self,
        audio1_data: np.ndarray,
        audio1_rate: int,
        audio2_data: np.ndarray,
        audio2_rate: int,
        max_tokens: int = DEFAULT_MAX_TOKENS,
        guidance_scale: float = DEFAULT_GUIDANCE_SCALE
    ) -> np.ndarray:
        """
        èåˆä¸¤ä¸ªéŸ³é¢‘ç”Ÿæˆï¼ˆä¸¤æ­¥æ³•ï¼‰
        
        Args:
            audio1_data: ç¬¬ä¸€ä¸ªéŸ³é¢‘æ•°æ®
            audio1_rate: ç¬¬ä¸€ä¸ªéŸ³é¢‘é‡‡æ ·ç‡
            audio2_data: ç¬¬äºŒä¸ªéŸ³é¢‘æ•°æ®
            audio2_rate: ç¬¬äºŒä¸ªéŸ³é¢‘é‡‡æ ·ç‡
            max_tokens: ç”Ÿæˆé•¿åº¦
            guidance_scale: å¼•å¯¼ç³»æ•°
            
        Returns:
            èåˆåçš„éŸ³é¢‘æ•°æ®
        """
        print("=" * 50)
        print("ä¸¤æ­¥èåˆç”Ÿæˆ")
        print("=" * 50)
        
        # ç¬¬ä¸€æ­¥ï¼šåŸºäºç¬¬ä¸€é¦–æ­Œç”Ÿæˆ
        print("\nç¬¬ 1 æ­¥ï¼šå­¦ä¹ ç¬¬ä¸€é¦–æ­Œçš„é£æ ¼")
        intermediate = self.generate_from_single(
            audio1_data,
            audio1_rate,
            PROMPT_STEP1,
            max_tokens=max_tokens // 2,  # ç¬¬ä¸€æ­¥ç”Ÿæˆä¸€åŠé•¿åº¦
            guidance_scale=guidance_scale
        )
        
        # ç¬¬äºŒæ­¥ï¼šèå…¥ç¬¬äºŒé¦–æ­Œçš„ç‰¹ç‚¹
        print("\nç¬¬ 2 æ­¥ï¼šèå…¥ç¬¬äºŒé¦–æ­Œçš„ç‰¹ç‚¹")
        final = self.generate_from_single(
            intermediate,
            self.model.config.audio_encoder.sampling_rate,
            PROMPT_STEP2,
            max_tokens=max_tokens,
            guidance_scale=guidance_scale
        )
        
        print("\n" + "=" * 50)
        print("âœ… èåˆå®Œæˆï¼")
        print("=" * 50)
        
        return final
    
    def save_audio(self, audio_data: np.ndarray, filepath: str):
        """
        ä¿å­˜éŸ³é¢‘
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            filepath: ä¿å­˜è·¯å¾„
        """
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        sample_rate = self.model.config.audio_encoder.sampling_rate
        wavfile.write(filepath, rate=sample_rate, data=audio_data)
        
        print(f"ğŸ’¾ éŸ³é¢‘å·²ä¿å­˜: {filepath}")
    
    def cleanup(self):
        """æ¸…ç† GPU å†…å­˜"""
        if self.device == "cuda":
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPU ç¼“å­˜å·²æ¸…ç†")

